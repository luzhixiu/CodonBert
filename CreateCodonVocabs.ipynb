{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=\"./Data/s288c.fasta.rank.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(inputFile)\n",
    "lines=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 1 1 3 1 3 3 1 1 1 1 1 2 3 3 1 3 1 1 1 2 1 1 3 2 1 4 1 1 1 3 2 3 1 1 3 1 1 2 2 3 1 2 2 1 2 1 3 3 2 1 1 1 3 2 1 3 1 1 1 1 3 1 2 2 2 1 2 2 3 3 1 1 3 1 1 1 1 2 1 4 3 1 1 3 3 1 1 1 1 2 4 5 1 2 2 1 3 3 4 3 1 3 4 2 2 1 3 \n",
      "\n",
      "3311 3133 1111 1233 1311 1211 3214 1113 2311 3112 2312 2121 3321 1132 1311 1131 2221 2233 1131 1112 1431 1331 1112 4512 2133 4313 4221\n"
     ]
    }
   ],
   "source": [
    "#start with fixed size vocabs\n",
    "def convertLineToWords(line,n):\n",
    "    line= line.replace(' ','')\n",
    "    wordList=[]\n",
    "    for i in range(0,len(line)-n,n):\n",
    "        wordList.append(line[i:i+n])\n",
    "    sentence=\" \".join(wordList)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "testline=lines[0]\n",
    "print(testline)\n",
    "print(convertLineToWords(testline,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceList=[]\n",
    "for line in lines:\n",
    "    sentenceList.append(convertLineToWords(line,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"s288c_words_4.train.txt\", 'w')\n",
    "\n",
    "for sentence in sentenceList[:5000]:\n",
    "    output_file.write(sentence+'\\n')\n",
    "\n",
    "output_file.close()\n",
    "\n",
    "\n",
    "output_test_file = open(\"s288c_words_4.test.txt\", 'w')\n",
    "\n",
    "for sentence in sentenceList[5000:]:\n",
    "    output_test_file.write(sentence+'\\n')\n",
    "\n",
    "output_test_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fast_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-26ea3f2d9640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfast_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"S288C_WordPiece_Tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fast_tokenizer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
